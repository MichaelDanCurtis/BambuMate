---
phase: 03-filament-scraping
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src-tauri/src/scraper/mod.rs
  - src-tauri/src/scraper/cache.rs
  - src-tauri/src/scraper/adapters/mod.rs
  - src-tauri/src/scraper/adapters/polymaker.rs
  - src-tauri/src/scraper/adapters/esun.rs
  - src-tauri/src/scraper/adapters/hatchbox.rs
  - src-tauri/src/scraper/adapters/overture.rs
  - src-tauri/src/scraper/adapters/inland.rs
  - src-tauri/src/scraper/adapters/prusament.rs
  - src-tauri/src/scraper/adapters/sunlu.rs
  - src-tauri/src/scraper/adapters/bambu.rs
  - src-tauri/src/scraper/adapters/creality.rs
  - src-tauri/src/scraper/adapters/elegoo.rs
  - src-tauri/src/scraper/adapters/spoolscout.rs
  - src-tauri/src/commands/mod.rs
  - src-tauri/src/commands/scraper.rs
  - src-tauri/src/lib.rs
autonomous: true

must_haves:
  truths:
    - "User can search for 'Polymaker PLA Pro' and get structured FilamentSpecs back"
    - "Scraper covers 10 brands: Polymaker, eSUN, Hatchbox, Overture, Inland, Prusament, SUNLU, Bambu, Creality, ELEGOO"
    - "Second lookup for same filament returns instantly from SQLite cache"
    - "Cache entries expire after 30 days and trigger re-fetch"
    - "SpoolScout is used as fallback when manufacturer page yields no specs"
    - "search_filament Tauri command is callable from frontend"
  artifacts:
    - path: "src-tauri/src/scraper/cache.rs"
      provides: "SQLite cache with 30-day TTL"
      contains: "pub struct FilamentCache"
    - path: "src-tauri/src/scraper/adapters/mod.rs"
      provides: "BrandAdapter trait and adapter registry"
      contains: "pub trait BrandAdapter"
    - path: "src-tauri/src/commands/scraper.rs"
      provides: "Tauri commands for filament search"
      contains: "pub async fn search_filament"
    - path: "src-tauri/src/scraper/mod.rs"
      provides: "Pipeline orchestrator search_filament function"
      contains: "pub async fn search_filament"
  key_links:
    - from: "src-tauri/src/scraper/mod.rs"
      to: "src-tauri/src/scraper/cache.rs"
      via: "checks cache before fetching, stores result after extraction"
      pattern: "cache\\.get|cache\\.put"
    - from: "src-tauri/src/scraper/mod.rs"
      to: "src-tauri/src/scraper/adapters/mod.rs"
      via: "finds adapter for brand, resolves URLs"
      pattern: "find_adapter|resolve_urls"
    - from: "src-tauri/src/scraper/mod.rs"
      to: "src-tauri/src/scraper/extraction.rs"
      via: "passes fetched page text to LLM extraction"
      pattern: "extract_specs"
    - from: "src-tauri/src/commands/scraper.rs"
      to: "src-tauri/src/scraper/mod.rs"
      via: "Tauri command calls pipeline orchestrator"
      pattern: "scraper::search_filament"
    - from: "src-tauri/src/lib.rs"
      to: "src-tauri/src/commands/scraper.rs"
      via: "registered in invoke_handler"
      pattern: "commands::scraper::search_filament"
---

<objective>
Build the brand adapters for 10+ manufacturers, SQLite caching with 30-day TTL, the pipeline orchestrator that ties everything together (cache -> adapter -> fetch -> extract -> validate -> cache), and Tauri commands to expose filament search to the frontend.

Purpose: This plan completes Phase 3 by wiring the extraction engine (from Plan 01) into a usable end-to-end pipeline. After this plan, a user can call `search_filament("Polymaker PLA Pro")` and get back structured, validated, cached specs.

Output: Complete scraper pipeline callable from Tauri frontend. 10 brand adapters + SpoolScout fallback. SQLite cache. Tauri commands registered.
</objective>

<execution_context>
@/Users/michaelcurtis/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelcurtis/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-filament-scraping/03-RESEARCH.md
@.planning/phases/03-filament-scraping/03-01-SUMMARY.md

@src-tauri/src/lib.rs
@src-tauri/src/commands/mod.rs
@src-tauri/src/commands/models.rs
@src-tauri/src/scraper/mod.rs
@src-tauri/src/scraper/types.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: SQLite cache with TTL and brand adapters for 10+ manufacturers</name>
  <files>
    src-tauri/src/scraper/cache.rs
    src-tauri/src/scraper/adapters/mod.rs
    src-tauri/src/scraper/adapters/polymaker.rs
    src-tauri/src/scraper/adapters/esun.rs
    src-tauri/src/scraper/adapters/hatchbox.rs
    src-tauri/src/scraper/adapters/overture.rs
    src-tauri/src/scraper/adapters/inland.rs
    src-tauri/src/scraper/adapters/prusament.rs
    src-tauri/src/scraper/adapters/sunlu.rs
    src-tauri/src/scraper/adapters/bambu.rs
    src-tauri/src/scraper/adapters/creality.rs
    src-tauri/src/scraper/adapters/elegoo.rs
    src-tauri/src/scraper/adapters/spoolscout.rs
    src-tauri/src/scraper/mod.rs
  </files>
  <action>
    **1. Create `src-tauri/src/scraper/cache.rs`** with:
    - `FilamentCache` struct wrapping `rusqlite::Connection`.
    - `pub fn new(db_path: &Path) -> Result<Self>` -- opens/creates SQLite DB, runs CREATE TABLE IF NOT EXISTS for `filament_cache` table with columns: `query TEXT PRIMARY KEY`, `specs_json TEXT NOT NULL`, `source_url TEXT NOT NULL`, `cached_at TEXT NOT NULL`, `expires_at TEXT NOT NULL`. Also create index on expires_at.
    - `pub fn get(&self, query: &str) -> Result<Option<FilamentSpecs>>` -- query by normalized key (lowercase, trimmed), check expires_at > now (UTC RFC3339). Return None if expired or not found.
    - `pub fn put(&self, query: &str, specs: &FilamentSpecs, ttl_days: i64) -> Result<()>` -- INSERT OR REPLACE with normalized key, serialized specs JSON, source_url from specs, cached_at = now, expires_at = now + ttl_days.
    - `pub fn clear_expired(&self) -> Result<usize>` -- DELETE WHERE expires_at < now. Return count of deleted rows.
    - All rusqlite operations are synchronous. The caller (pipeline orchestrator) should wrap in `tokio::task::spawn_blocking` when calling from async context.
    - Normalize query keys: lowercase, trim whitespace, collapse multiple spaces to single space.

    **2. Create `src-tauri/src/scraper/adapters/mod.rs`** with:
    - `BrandAdapter` trait (see RESEARCH.md):
      ```rust
      pub trait BrandAdapter: Send + Sync {
          fn brand_name(&self) -> &str;
          fn brand_aliases(&self) -> Vec<&str> { vec![] }  // e.g., "bambulab" for "bambu"
          fn resolve_urls(&self, filament_name: &str) -> Vec<String>;
          fn search_url(&self, query: &str) -> Option<String> { None }
      }
      ```
    - `pub fn all_adapters() -> Vec<Box<dyn BrandAdapter>>` -- returns instances of all 10 brand adapters plus SpoolScout.
    - `pub fn find_adapter(filament_name: &str) -> Option<Box<dyn BrandAdapter>>` -- case-insensitive match of first word(s) in filament_name against brand_name() and brand_aliases(). e.g., "Polymaker PLA Pro" matches "polymaker". "Bambu Lab PLA Basic" matches "bambu" or "bambulab". If no brand match, return None (caller will use SpoolScout fallback).

    **3. Create individual brand adapter files.** Each implements BrandAdapter. URL patterns from RESEARCH.md:

    - **polymaker.rs**: brand "polymaker". resolve_urls: `https://us.polymaker.com/products/{slug}` where slug is the product name slugified (lowercase, spaces to hyphens). Also include SpoolScout URL as secondary.
    - **esun.rs**: brand "esun", aliases ["esun3d"]. resolve_urls: `https://www.esun3d.com/{slug}-product/` where slug is material name lowercased with hyphens. eSUN has specs in HTML -- highest confidence source.
    - **hatchbox.rs**: brand "hatchbox". resolve_urls: `https://www.hatchbox3d.com/products/{slug}`. Note: Hatchbox product pages lack specs (Tier 3 from research). Include SpoolScout URL.
    - **overture.rs**: brand "overture", aliases ["overture3d"]. resolve_urls: `https://overture3d.com/products/{slug}`. Also Tier 3 -- include SpoolScout.
    - **inland.rs**: brand "inland". resolve_urls: Inland is Micro Center's brand. Use SpoolScout as primary since Micro Center product pages have minimal specs.
    - **prusament.rs**: brand "prusament", aliases ["prusa"]. resolve_urls: `https://www.prusa3d.com/product/prusament-{slug}/`.
    - **sunlu.rs**: brand "sunlu". resolve_urls: `https://store.sunlu.com/products/{slug}`. SUNLU has specs in HTML (Tier 1).
    - **bambu.rs**: brand "bambu", aliases ["bambulab", "bambu lab"]. resolve_urls: `https://us.store.bambulab.com/products/{slug}`. Tier 2 -- basic specs on page but full data in TDS PDFs (skip PDF for Phase 3).
    - **creality.rs**: brand "creality". resolve_urls: `https://store.creality.com/products/{slug}`. Tier 3 -- include SpoolScout.
    - **elegoo.rs**: brand "elegoo". resolve_urls: `https://us.elegoo.com/products/{slug}`. Tier 3 -- specs loaded dynamically, use SpoolScout.
    - **spoolscout.rs**: brand "spoolscout" (not a real brand -- used as fallback). resolve_urls: `https://www.spoolscout.com/data-sheets/{brand}/{material}-{product}`. Also implement a `pub fn fallback_url(brand: &str, filament_name: &str) -> String` helper that other adapters can use.

    **URL slugification helper** (put in adapters/mod.rs): `fn slugify(name: &str) -> String` -- lowercase, replace spaces/underscores with hyphens, remove non-alphanumeric-or-hyphen chars, collapse multiple hyphens.

    **4. Update `src-tauri/src/scraper/mod.rs`** to add:
    ```rust
    pub mod cache;
    pub mod adapters;
    ```

    **Key implementation notes:**
    - Each adapter's resolve_urls should return 1-3 URLs in priority order. The pipeline will try them in order until one yields good extraction results (confidence > 0.3).
    - Brand matching should be generous -- "eSUN" and "esun" and "ESUN" all match the esun adapter.
    - The slugify function is critical -- it turns "PLA Pro" into "pla-pro" for URL construction.
    - Do NOT hardcode specific product IDs or SKUs -- use the slugified filament name to construct URLs. The URLs may 404, and that's okay -- the pipeline handles it.
  </action>
  <verify>
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo check -p bambumate-tauri` -- compiles.
    Write and run unit tests:
    - `find_adapter("Polymaker PLA Pro")` returns Some with brand_name "polymaker"
    - `find_adapter("eSUN PLA+")` returns Some with brand_name "esun"
    - `find_adapter("Bambu Lab PLA Basic")` returns Some with brand_name "bambu"
    - `find_adapter("Unknown Brand X")` returns None
    - `slugify("PLA Pro Silk")` returns "pla-pro-silk"
    - FilamentCache: put then get returns the same specs; get expired entry returns None
    - FilamentCache: get nonexistent key returns None
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo test -p bambumate-tauri -- scraper` -- all tests pass.
  </verify>
  <done>
    - SQLite cache creates DB, stores/retrieves FilamentSpecs, respects 30-day TTL
    - 10 brand adapters implemented with correct URL patterns
    - SpoolScout fallback adapter available for Tier 3 brands
    - find_adapter matches brands case-insensitively including aliases
    - All adapters return 1-3 candidate URLs per filament
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Pipeline orchestrator and Tauri commands</name>
  <files>
    src-tauri/src/scraper/mod.rs
    src-tauri/src/commands/scraper.rs
    src-tauri/src/commands/mod.rs
    src-tauri/src/lib.rs
  </files>
  <action>
    **1. Add the pipeline orchestrator to `src-tauri/src/scraper/mod.rs`:**

    Create `pub async fn search_filament(name: &str, provider: &str, model: &str, api_key: &str, cache_dir: &Path) -> Result<FilamentSpecs, String>` that implements the cache-first lookup flow:

    1. **Cache check:** Create `FilamentCache::new(cache_dir.join("filament_cache.db"))` (wrap in `spawn_blocking` since rusqlite is sync). Call `cache.get(name)`. If found and not expired, return immediately.
    2. **Resolve brand:** Call `adapters::find_adapter(name)`. If found, get URLs from adapter. If not found, construct a SpoolScout fallback URL directly.
    3. **Fetch and extract:** For each URL from the adapter (in order):
       a. Call `http_client.fetch_page(url)`. If it fails (404, robots.txt blocked, timeout), log warning and try next URL.
       b. Convert HTML to text: `ScraperHttpClient::html_to_text(&html)`.
       c. Call `extraction::extract_specs(&text, name, provider, model, api_key)`.
       d. If extraction returns specs with confidence > 0.3, accept them. If confidence <= 0.3 and more URLs remain, try next URL.
    4. **SpoolScout fallback:** If all manufacturer URLs failed or yielded low confidence, try SpoolScout URL(s) as a final attempt.
    5. **Validate:** Run `validation::validate_specs(&specs)` on the final result. Log any warnings but still return the specs.
    6. **Cache store:** Store successful result in cache with 30-day TTL (wrap in `spawn_blocking`).
    7. **Return:** Return the FilamentSpecs. If ALL sources failed, return an error explaining that no specs could be found for the given filament.

    Also add `pub async fn search_filament_cached_only(name: &str, cache_dir: &Path) -> Result<Option<FilamentSpecs>, String>` that only checks cache, no network. Useful for instant lookups.

    **2. Create `src-tauri/src/commands/scraper.rs`** with Tauri commands:

    - `#[tauri::command] pub async fn search_filament(app: tauri::AppHandle, filament_name: String) -> Result<FilamentSpecs, String>` that:
      1. Gets the configured AI provider and model from preferences (via `tauri_plugin_store` -- read `ai_provider` and `ai_model` keys, same pattern as commands/config.rs)
      2. Gets the API key from keychain (via `keyring::Entry` -- same pattern as commands/models.rs `get_key_for_provider`)
      3. Resolves the cache directory: `app.path().app_data_dir()` or similar Tauri path API
      4. Calls `crate::scraper::search_filament(&filament_name, &provider, &model, &api_key, &cache_dir).await`
      5. Returns the result

    - `#[tauri::command] pub async fn get_cached_filament(app: tauri::AppHandle, filament_name: String) -> Result<Option<FilamentSpecs>, String>` -- cache-only lookup, no network.

    - `#[tauri::command] pub async fn clear_filament_cache(app: tauri::AppHandle) -> Result<usize, String>` -- clears expired cache entries, returns count deleted.

    **3. Update `src-tauri/src/commands/mod.rs`** to add `pub mod scraper;`.

    **4. Update `src-tauri/src/lib.rs`** to register new commands in the invoke_handler:
    ```rust
    commands::scraper::search_filament,
    commands::scraper::get_cached_filament,
    commands::scraper::clear_filament_cache,
    ```

    **Important implementation details:**
    - The ScraperHttpClient should be created once per search_filament call (not per URL). It maintains rate limiting state across URLs.
    - Use `tokio::task::spawn_blocking` for ALL rusqlite operations (cache.get, cache.put, cache.clear_expired) to avoid blocking the async runtime.
    - For the Tauri app data directory, use `app.path().app_data_dir().map_err(|e| e.to_string())?` and create it if it doesn't exist with `std::fs::create_dir_all`.
    - Error messages should be user-friendly: "No specs found for 'XYZ'. Try checking the filament name spelling." not internal error dumps.
    - The preferences for ai_provider and ai_model default to "claude" and "claude-sonnet-4-20250514" if not set, matching the existing codebase patterns.
  </action>
  <verify>
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo check -p bambumate-tauri` -- compiles cleanly with no errors.
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo test -p bambumate-tauri -- scraper` -- all existing tests still pass.
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo build -p bambumate-tauri` -- full build succeeds.
    Verify the 3 new commands are registered in lib.rs invoke_handler (grep for `search_filament`, `get_cached_filament`, `clear_filament_cache`).
    Verify scraper module exports: `search_filament` function is accessible from `crate::scraper::search_filament`.
  </verify>
  <done>
    - Pipeline orchestrator implements full cache-first lookup flow: cache -> adapter -> fetch -> extract -> validate -> cache
    - Low-confidence results (<=0.3) trigger fallback to next URL or SpoolScout
    - 3 Tauri commands registered: search_filament, get_cached_filament, clear_filament_cache
    - Commands read AI provider/model from preferences and API key from keychain
    - All rusqlite operations wrapped in spawn_blocking
    - Full project builds successfully
    - All tests pass
  </done>
</task>

</tasks>

<verification>
1. `cargo build -p bambumate-tauri` succeeds with no errors or warnings
2. `cargo test -p bambumate-tauri -- scraper` runs all scraper tests and they pass
3. 10 brand adapters exist and are registered in all_adapters()
4. find_adapter correctly resolves all 10 brands (case-insensitive)
5. SQLite cache: put/get works, TTL expiration works, expired entries cleaned up
6. Pipeline orchestrator: cache hit returns instantly, cache miss triggers fetch+extract+cache
7. 3 Tauri commands registered in lib.rs invoke_handler
8. Grep confirms no hardcoded API keys anywhere in scraper module
</verification>

<success_criteria>
- Complete end-to-end pipeline: filament name -> structured specs (via cache or live extraction)
- 10 brand adapters + SpoolScout fallback covering all required brands
- SQLite cache with 30-day TTL reduces repeat lookups to instant
- Tauri commands expose search_filament to frontend
- All rusqlite operations use spawn_blocking (no async runtime blocking)
- All code compiles and tests pass
- Phase 3 success criteria met: user can search, 10+ brands, validation works, cache works, robots.txt respected
</success_criteria>

<output>
After completion, create `.planning/phases/03-filament-scraping/03-02-SUMMARY.md`
</output>
