---
phase: 03-filament-scraping
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src-tauri/Cargo.toml
  - src-tauri/src/lib.rs
  - src-tauri/src/error.rs
  - src-tauri/src/scraper/mod.rs
  - src-tauri/src/scraper/types.rs
  - src-tauri/src/scraper/validation.rs
  - src-tauri/src/scraper/http_client.rs
  - src-tauri/src/scraper/extraction.rs
  - src-tauri/src/scraper/prompts.rs
autonomous: true

must_haves:
  truths:
    - "FilamentSpecs struct can serialize/deserialize to/from JSON matching the LLM extraction schema"
    - "Physical constraint validation rejects PLA at 350C nozzle temp and passes PLA at 210C"
    - "HTTP client enforces max 1 request/second per domain via sleep-based rate limiting"
    - "HTTP client checks robots.txt before fetching any page and blocks disallowed URLs"
    - "LLM extraction sends HTML-to-text content with structured output schema to configured AI provider and returns parsed FilamentSpecs"
    - "Extraction prompt instructs LLM to return null for missing fields, never guess"
  artifacts:
    - path: "src-tauri/src/scraper/types.rs"
      provides: "FilamentSpecs, FilamentType enum, ValidationWarning"
      contains: "pub struct FilamentSpecs"
    - path: "src-tauri/src/scraper/validation.rs"
      provides: "Physical constraint validation per material type"
      contains: "pub fn validate_specs"
    - path: "src-tauri/src/scraper/http_client.rs"
      provides: "Rate-limited HTTP client with robots.txt checking"
      contains: "pub struct ScraperHttpClient"
    - path: "src-tauri/src/scraper/extraction.rs"
      provides: "LLM-based structured extraction engine"
      contains: "pub async fn extract_specs"
    - path: "src-tauri/src/scraper/prompts.rs"
      provides: "Extraction prompt builder and JSON schema definition"
      contains: "pub fn filament_specs_json_schema"
  key_links:
    - from: "src-tauri/src/scraper/extraction.rs"
      to: "src-tauri/src/scraper/types.rs"
      via: "deserializes LLM response into FilamentSpecs"
      pattern: "serde_json::from_str.*FilamentSpecs"
    - from: "src-tauri/src/scraper/extraction.rs"
      to: "src-tauri/src/scraper/prompts.rs"
      via: "uses prompt builder and JSON schema"
      pattern: "build_extraction_prompt|filament_specs_json_schema"
    - from: "src-tauri/src/scraper/extraction.rs"
      to: "src-tauri/src/scraper/validation.rs"
      via: "validates extracted specs before returning"
      pattern: "validate_specs"
    - from: "src-tauri/src/scraper/extraction.rs"
      to: "src-tauri/src/scraper/http_client.rs"
      via: "uses HTTP client for page fetching"
      pattern: "ScraperHttpClient|fetch_page"
---

<objective>
Build the core filament scraping infrastructure: data types, physical constraint validation, rate-limited HTTP client with robots.txt compliance, and the LLM-assisted extraction engine that converts manufacturer web pages into structured FilamentSpecs.

Purpose: This is the foundation of the scraping pipeline. Every filament lookup flows through these components: HTTP fetch -> HTML-to-text -> LLM extraction -> validation. Without this, no brand adapters or caching can function.

Output: A working `scraper` module with types, validation, HTTP client, and extraction engine. Not yet wired to Tauri commands (that's Plan 02), but fully testable via unit tests.
</objective>

<execution_context>
@/Users/michaelcurtis/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelcurtis/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-filament-scraping/03-RESEARCH.md

@src-tauri/Cargo.toml
@src-tauri/src/lib.rs
@src-tauri/src/error.rs
@src-tauri/src/commands/models.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies, create scraper types, and implement physical constraint validation</name>
  <files>
    src-tauri/Cargo.toml
    src-tauri/src/lib.rs
    src-tauri/src/error.rs
    src-tauri/src/scraper/mod.rs
    src-tauri/src/scraper/types.rs
    src-tauri/src/scraper/validation.rs
  </files>
  <action>
    **1. Add new dependencies to Cargo.toml:**
    ```toml
    scraper = "0.18"
    html2text = "0.16"
    texting_robots = "0.2"
    rusqlite = { version = "0.38", features = ["bundled"] }
    chrono = { version = "0.4", features = ["serde"] }
    url = "2.5"
    tokio = { version = "1", features = ["time"] }
    ```
    Note: reqwest and serde_json are already present. The `tokio` dep may already be transitive via tauri, but add it explicitly for the `time` feature needed for rate limiting sleep.

    **2. Add `pub mod scraper;` to `src-tauri/src/lib.rs`** (alongside existing `mod commands; mod error; pub mod profile;`).

    **3. Add a `Scraper(String)` variant to `BambuMateError` in `src-tauri/src/error.rs`** following the same pattern as the existing Profile variant:
    ```rust
    #[error("Scraper error: {0}")]
    Scraper(String),
    ```

    **4. Create `src-tauri/src/scraper/mod.rs`** with:
    ```rust
    pub mod types;
    pub mod validation;
    pub mod http_client;
    pub mod extraction;
    pub mod prompts;
    ```

    **5. Create `src-tauri/src/scraper/types.rs`** with:
    - `FilamentSpecs` struct (see RESEARCH.md for exact fields): name, brand, material (String), nozzle_temp_min/max (Option<u16>), bed_temp_min/max (Option<u16>), max_speed_mm_s (Option<u16>), fan_speed_percent (Option<u8>), retraction_distance_mm (Option<f32>), retraction_speed_mm_s (Option<u16>), density_g_cm3 (Option<f32>), diameter_mm (Option<f32>), source_url (String), extraction_confidence (f32). Derive Debug, Clone, Serialize, Deserialize.
    - `MaterialType` enum: PLA, PETG, ABS, ASA, TPU, Nylon, PC, PVA, HIPS, Other(String). Implement `from_str` that does case-insensitive matching including variants like "PLA+", "PLA Pro" -> PLA, "PA6"/"PA12" -> Nylon, "TPE" -> TPU, "POLYCARBONATE" -> PC.
    - `ValidationWarning` struct: field (String), message (String), value (String). Derive Debug, Clone, Serialize.

    **6. Create `src-tauri/src/scraper/validation.rs`** with:
    - `MaterialConstraints` struct: nozzle_temp_min/max (u16), bed_temp_min/max (u16). NOT public constructor -- only via `constraints_for_material`.
    - `pub fn constraints_for_material(material: &MaterialType) -> MaterialConstraints` -- return per-material ranges as specified in RESEARCH.md (PLA: 180-235/0-70, PETG: 210-260/40-100, ABS: 210-270/70-120, ASA: 220-270/80-120, TPU: 200-250/20-70, Nylon: 230-300/50-100, PC: 250-320/90-150, PVA: 170-220/30-65, HIPS: 210-260/80-115, Other/fallback: 150-400/0-120).
    - `pub fn validate_specs(specs: &FilamentSpecs) -> Vec<ValidationWarning>` -- check all temperature fields against material constraints, check retraction_distance_mm is 0.0-15.0, check retraction_speed_mm_s is 0-100, check fan_speed_percent is 0-100, check diameter_mm is 1.0-3.5 if present. Return warnings (not errors) for each violation. Parse material from specs.material using MaterialType.
  </action>
  <verify>
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo check -p bambumate-tauri` -- should compile with no errors.
    Write and run unit tests in validation.rs:
    - Test PLA specs with nozzle_temp_max=210 passes validation (no warnings)
    - Test PLA specs with nozzle_temp_max=350 produces a warning
    - Test MaterialType parsing: "PLA+", "pla pro", "PETG", "PA6", "TPE" all resolve correctly
    - Test Other material uses permissive fallback ranges
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo test -p bambumate-tauri -- scraper` -- all tests pass.
  </verify>
  <done>
    - Cargo.toml has all 7 new dependencies added
    - scraper module declared in lib.rs
    - BambuMateError has Scraper variant
    - FilamentSpecs struct serializes/deserializes correctly
    - MaterialType covers all 10 types with case-insensitive parsing
    - validate_specs catches out-of-range temps, retraction, fan speed
    - All unit tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Build rate-limited HTTP client with robots.txt and LLM extraction engine</name>
  <files>
    src-tauri/src/scraper/http_client.rs
    src-tauri/src/scraper/extraction.rs
    src-tauri/src/scraper/prompts.rs
  </files>
  <action>
    **1. Create `src-tauri/src/scraper/http_client.rs`** with:
    - `RateLimiter` struct: `last_request: Mutex<HashMap<String, Instant>>`, `min_interval: Duration`. Constructor takes `requests_per_second: f64`. Method `pub async fn wait_for_domain(&self, url: &str) -> Result<()>` extracts domain via `url::Url::parse`, checks elapsed time since last request to that domain, sleeps if needed using `tokio::time::sleep`, updates last_request map.
    - `RobotsCache` struct: `cache: Mutex<HashMap<String, (Robot, Instant)>>`, TTL of 1 hour. Method `pub async fn is_allowed(&self, client: &reqwest::Client, url: &str) -> Result<bool>` fetches robots.txt for the domain (once, then cached), checks if the specific URL path is allowed. Use `texting_robots::Robot::new("BambuMate/1.0", &body)`. Handle 404 (no robots.txt) as "all allowed". Method should also return crawl-delay from robots.txt if set.
    - `ScraperHttpClient` struct: wraps `reqwest::Client`, `RateLimiter`, `RobotsCache`. Constructor `pub fn new() -> Self` with default 1 req/sec rate limit and User-Agent "BambuMate/1.0". Public methods:
      - `pub async fn fetch_page(&self, url: &str) -> Result<String>` -- checks robots.txt, waits for rate limit (using max of 1sec and crawl-delay if set), fetches page, returns HTML body. Set timeout of 30 seconds. Return descriptive error if robots.txt blocks the URL.
      - `pub fn html_to_text(html: &str) -> String` -- uses `html2text::from_read(html.as_bytes(), 120)` to convert HTML to plain text. This is a static utility method.

    **2. Create `src-tauri/src/scraper/prompts.rs`** with:
    - `pub fn filament_specs_json_schema() -> serde_json::Value` -- returns the JSON schema as specified in RESEARCH.md. All spec fields use `["integer", "null"]` or `["number", "null"]` types. Required fields: name, brand, material, plus all spec fields (with null allowed), plus confidence.
    - `pub fn build_extraction_prompt(filament_name: &str, page_text: &str) -> String` -- returns the extraction prompt from RESEARCH.md. Key rules: "Only extract values explicitly stated in the text below", "If a value is NOT present, return null", "Do NOT guess or use general knowledge", confidence scoring guidelines.

    **3. Create `src-tauri/src/scraper/extraction.rs`** with:
    - `pub async fn extract_specs(page_text: &str, filament_name: &str, provider: &str, model: &str, api_key: &str) -> Result<FilamentSpecs>` that:
      1. Builds prompt via `build_extraction_prompt(filament_name, page_text)`
      2. Gets JSON schema via `filament_specs_json_schema()`
      3. Calls the appropriate AI API based on `provider`:
         - For "claude": POST to `https://api.anthropic.com/v1/messages` with `output_config.format` containing `json_schema` (see RESEARCH.md for exact format). Headers: `x-api-key`, `anthropic-version: 2023-06-01`, `content-type: application/json`. Extract text from `response.content[0].text`.
         - For "openai": POST to `https://api.openai.com/v1/chat/completions` with `response_format` containing `json_schema` with `strict: true` (see RESEARCH.md). Header: `Authorization: Bearer {key}`. Extract text from `response.choices[0].message.content`.
         - For "kimi": Same as openai but POST to `https://api.moonshot.cn/v1/chat/completions`. Use standard JSON mode with prompt-based enforcement (not json_schema) as Kimi structured output support is unverified.
         - For "openrouter": Same as openai but POST to `https://openrouter.ai/api/v1/chat/completions`.
      4. Deserializes the response text into `FilamentSpecs` via `serde_json::from_str`
      5. Runs `validate_specs` on the result and attaches warnings (log them via tracing::warn but still return the specs)
      6. Returns the FilamentSpecs

    **Important patterns to follow:**
    - Match the existing reqwest usage pattern from `commands/models.rs` (create client, build request, check status, parse response)
    - Use `tracing::info` and `tracing::warn` for logging, matching existing codebase style
    - Return `Result<FilamentSpecs, String>` for errors (matching Tauri command pattern) or use anyhow internally and convert at boundaries
    - The `api_key` parameter will be retrieved from keychain at the command layer (Plan 02), not here. This module receives the key as a parameter.
    - Do NOT create a new reqwest::Client per call in extraction.rs -- accept it as a parameter or use the ScraperHttpClient's inner client. Create a new client for each provider call is acceptable for now since extractions are infrequent.
  </action>
  <verify>
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo check -p bambumate-tauri` -- should compile with no errors.
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo test -p bambumate-tauri -- scraper` -- all tests pass.
    Write unit tests:
    - `prompts::filament_specs_json_schema()` returns valid JSON with all expected fields
    - `prompts::build_extraction_prompt("Polymaker PLA Pro", "some text")` contains the filament name and the source text and the anti-hallucination rules
    - `http_client::ScraperHttpClient::html_to_text("<h1>Hello</h1><p>World</p>")` returns text containing "Hello" and "World" without HTML tags
    - RateLimiter enforces delay (test that two sequential calls to same domain have at least min_interval between them -- use Instant::now() before/after)
  </verify>
  <done>
    - ScraperHttpClient fetches pages with rate limiting (1 req/sec per domain) and robots.txt checking
    - html_to_text converts HTML to clean plain text
    - LLM extraction supports 4 providers (claude, openai, kimi, openrouter) with structured output schemas
    - Extraction prompt explicitly forbids hallucination ("return null, do NOT guess")
    - All code compiles and tests pass
  </done>
</task>

</tasks>

<verification>
1. `cargo check -p bambumate-tauri` compiles cleanly
2. `cargo test -p bambumate-tauri -- scraper` runs all scraper module tests and they pass
3. The scraper module structure exists: mod.rs, types.rs, validation.rs, http_client.rs, extraction.rs, prompts.rs
4. FilamentSpecs round-trips through serde_json (serialize then deserialize produces equal struct)
5. Validation catches PLA at 350C (warning produced) and passes PLA at 210C (no warning)
</verification>

<success_criteria>
- scraper module compiles and all tests pass
- FilamentSpecs type matches the JSON schema used for LLM extraction
- Physical constraint validation covers all 10 material types with per-material ranges
- HTTP client rate-limits to 1 req/sec per domain
- robots.txt is checked before every page fetch
- LLM extraction supports all 4 configured AI providers
- No hardcoded API keys anywhere -- keys passed as parameters
</success_criteria>

<output>
After completion, create `.planning/phases/03-filament-scraping/03-01-SUMMARY.md`
</output>
