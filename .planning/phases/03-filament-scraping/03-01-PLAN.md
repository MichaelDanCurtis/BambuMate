---
phase: 03-filament-scraping
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src-tauri/Cargo.toml
  - src-tauri/src/lib.rs
  - src-tauri/src/error.rs
  - src-tauri/src/scraper/mod.rs
  - src-tauri/src/scraper/types.rs
  - src-tauri/src/scraper/validation.rs
  - src-tauri/src/scraper/http_client.rs
  - src-tauri/src/scraper/extraction.rs
  - src-tauri/src/scraper/prompts.rs
autonomous: true

must_haves:
  truths:
    - "FilamentSpecs struct can serialize/deserialize to/from JSON matching the LLM extraction schema"
    - "Physical constraint validation rejects PLA at 350C nozzle temp and passes PLA at 210C"
    - "HTTP client enforces max 1 request/second per domain via sleep-based rate limiting"
    - "HTTP client checks robots.txt before fetching any page and blocks disallowed URLs"
    - "LLM extraction sends HTML-to-text content with structured output schema to configured AI provider and returns parsed FilamentSpecs"
    - "Extraction prompt instructs LLM to return null for missing fields, never guess"
  artifacts:
    - path: "src-tauri/src/scraper/types.rs"
      provides: "FilamentSpecs, FilamentType enum, ValidationWarning"
      contains: "pub struct FilamentSpecs"
    - path: "src-tauri/src/scraper/validation.rs"
      provides: "Physical constraint validation per material type"
      contains: "pub fn validate_specs"
    - path: "src-tauri/src/scraper/http_client.rs"
      provides: "Rate-limited HTTP client with robots.txt checking"
      contains: "pub struct ScraperHttpClient"
    - path: "src-tauri/src/scraper/extraction.rs"
      provides: "LLM-based structured extraction engine"
      contains: "pub async fn extract_specs"
    - path: "src-tauri/src/scraper/prompts.rs"
      provides: "Extraction prompt builder and JSON schema definition"
      contains: "pub fn filament_specs_json_schema"
  key_links:
    - from: "src-tauri/src/scraper/extraction.rs"
      to: "src-tauri/src/scraper/types.rs"
      via: "deserializes LLM response into FilamentSpecs"
      pattern: "serde_json::from_str.*FilamentSpecs"
    - from: "src-tauri/src/scraper/extraction.rs"
      to: "src-tauri/src/scraper/prompts.rs"
      via: "uses prompt builder and JSON schema"
      pattern: "build_extraction_prompt|filament_specs_json_schema"
    - from: "src-tauri/src/scraper/extraction.rs"
      to: "src-tauri/src/scraper/validation.rs"
      via: "validates extracted specs before returning"
      pattern: "validate_specs"
    - from: "src-tauri/src/scraper/extraction.rs"
      to: "src-tauri/src/scraper/http_client.rs"
      via: "uses HTTP client for page fetching"
      pattern: "ScraperHttpClient|fetch_page"
---

<objective>
Build the core filament scraping infrastructure: data types, physical constraint validation, rate-limited HTTP client with robots.txt compliance, and the LLM-assisted extraction engine that converts manufacturer web pages into structured FilamentSpecs.

Purpose: This is the foundation of the scraping pipeline. Every filament lookup flows through these components: HTTP fetch -> HTML-to-text -> LLM extraction -> validation. Without this, no brand adapters or caching can function.

Output: A working `scraper` module with types, validation, HTTP client, and extraction engine. Not yet wired to Tauri commands (that's Plan 02), but fully testable via unit tests.
</objective>

<execution_context>
@/Users/michaelcurtis/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelcurtis/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-filament-scraping/03-RESEARCH.md

@src-tauri/Cargo.toml
@src-tauri/src/lib.rs
@src-tauri/src/error.rs
@src-tauri/src/commands/models.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies, create scraper types, and implement physical constraint validation</name>
  <files>
    src-tauri/Cargo.toml
    src-tauri/src/lib.rs
    src-tauri/src/error.rs
    src-tauri/src/scraper/mod.rs
    src-tauri/src/scraper/types.rs
    src-tauri/src/scraper/validation.rs
  </files>
  <action>
    **1. Add new dependencies to Cargo.toml:**
    ```toml
    scraper = "0.18"
    html2text = "0.16"
    texting_robots = "0.2"
    rusqlite = { version = "0.38", features = ["bundled"] }
    chrono = { version = "0.4", features = ["serde"] }
    url = "2.5"
    tokio = { version = "1", features = ["time"] }
    ```
    Note: reqwest and serde_json are already present. The `tokio` dep may already be transitive via tauri, but add it explicitly for the `time` feature needed for rate limiting sleep.

    **2. Add `pub mod scraper;` to `src-tauri/src/lib.rs`** (alongside existing `mod commands; mod error; pub mod profile;`).

    **3. Add a `Scraper(String)` variant to `BambuMateError` in `src-tauri/src/error.rs`** following the same pattern as the existing Profile variant:
    ```rust
    #[error("Scraper error: {0}")]
    Scraper(String),
    ```

    **4. Create `src-tauri/src/scraper/mod.rs`** with:
    ```rust
    pub mod types;
    pub mod validation;
    pub mod http_client;
    pub mod extraction;
    pub mod prompts;
    ```

    **5. Create `src-tauri/src/scraper/types.rs`** with:
    - `FilamentSpecs` struct (see RESEARCH.md for exact fields): name, brand, material (String), nozzle_temp_min/max (Option<u16>), bed_temp_min/max (Option<u16>), max_speed_mm_s (Option<u16>), fan_speed_percent (Option<u8>), retraction_distance_mm (Option<f32>), retraction_speed_mm_s (Option<u16>), density_g_cm3 (Option<f32>), diameter_mm (Option<f32>), source_url (String), extraction_confidence (f32). Derive Debug, Clone, Serialize, Deserialize.
    - `MaterialType` enum: PLA, PETG, ABS, ASA, TPU, Nylon, PC, PVA, HIPS, Other(String). Derive Debug, Clone, Serialize, Deserialize.
    - Implement `MaterialType::from_str(input: &str) -> MaterialType` as a method (not the std FromStr trait -- just a plain associated fn) that performs **case-insensitive substring matching** using the following logic:
      1. Convert input to uppercase for comparison.
      2. Check substrings in this priority order (earlier matches win):
         - Contains "PLA" -> `MaterialType::PLA` (catches "PLA", "PLA+", "PLA Pro", "PolyLite PLA")
         - Contains "PETG" -> `MaterialType::PETG` (catches "PETG", "PETG-CF")
         - Contains "ABS" -> `MaterialType::ABS` (catches "ABS", "ABS+")
         - Contains "ASA" -> `MaterialType::ASA`
         - Contains "TPU" or "TPE" -> `MaterialType::TPU` (catches "TPU 95A", "TPE")
         - Contains "PA" or "NYLON" -> `MaterialType::Nylon` (catches "PA6", "PA12", "PA6-CF", "Nylon")
         - Contains "PC" or "POLYCARBONATE" -> `MaterialType::PC` (catches "PC", "PC-ABS", "Polycarbonate")
         - Contains "PVA" -> `MaterialType::PVA`
         - Contains "HIPS" -> `MaterialType::HIPS`
         - No match -> `MaterialType::Other(input.to_string())`
      3. **Important ordering note:** "PA" must be checked AFTER "PLA" to avoid "PLA" matching as "PA". Similarly, "PC" check should not false-match "PC" substring in unrelated strings -- but in practice filament material strings are short enough that this is not a concern.
    - `ValidationWarning` struct: field (String), message (String), value (String). Derive Debug, Clone, Serialize.

    **6. Create `src-tauri/src/scraper/validation.rs`** with:
    - `MaterialConstraints` struct: nozzle_temp_min/max (u16), bed_temp_min/max (u16). NOT public constructor -- only via `constraints_for_material`.
    - `pub fn constraints_for_material(material: &MaterialType) -> MaterialConstraints` -- return per-material ranges as specified in RESEARCH.md (PLA: 180-235/0-70, PETG: 210-260/40-100, ABS: 210-270/70-120, ASA: 220-270/80-120, TPU: 200-250/20-70, Nylon: 230-300/50-100, PC: 250-320/90-150, PVA: 170-220/30-65, HIPS: 210-260/80-115, Other/fallback: 150-400/0-120).
    - `pub fn validate_specs(specs: &FilamentSpecs) -> Vec<ValidationWarning>` -- check all temperature fields against material constraints, check retraction_distance_mm is 0.0-15.0, check retraction_speed_mm_s is 0-100, check fan_speed_percent is 0-100, check diameter_mm is 1.0-3.5 if present. Return warnings (not errors) for each violation. Parse material from specs.material using MaterialType.
  </action>
  <verify>
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo check -p bambumate-tauri` -- should compile with no errors.
    Write and run unit tests in validation.rs:
    - Test PLA specs with nozzle_temp_max=210 passes validation (no warnings)
    - Test PLA specs with nozzle_temp_max=350 produces a warning
    - Test MaterialType parsing: "PLA+", "pla pro", "PETG", "PA6", "TPE" all resolve correctly
    - Test Other material uses permissive fallback ranges
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo test -p bambumate-tauri -- scraper` -- all tests pass.
  </verify>
  <done>
    - Cargo.toml has all 7 new dependencies added
    - scraper module declared in lib.rs
    - BambuMateError has Scraper variant
    - FilamentSpecs struct serializes/deserializes correctly
    - MaterialType covers all 10 types with case-insensitive substring matching (priority-ordered to avoid false positives)
    - validate_specs catches out-of-range temps, retraction, fan speed
    - All unit tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Build rate-limited HTTP client with robots.txt and LLM extraction engine</name>
  <!-- Scope note: This task covers 3 files with 4 AI provider integrations. Accepted because all 4 providers
       follow the same pattern (request builder + response parser) with only URL/header/body format differences.
       The commonality makes the scope manageable within a single task. -->
  <files>
    src-tauri/src/scraper/http_client.rs
    src-tauri/src/scraper/extraction.rs
    src-tauri/src/scraper/prompts.rs
  </files>
  <action>
    **1. Create `src-tauri/src/scraper/http_client.rs`** with:
    - `RateLimiter` struct: `last_request: Mutex<HashMap<String, Instant>>`, `min_interval: Duration`. Constructor takes `requests_per_second: f64`. Method `pub async fn wait_for_domain(&self, url: &str) -> Result<()>` extracts domain via `url::Url::parse`, checks elapsed time since last request to that domain, sleeps if needed using `tokio::time::sleep`, updates last_request map.
    - `RobotsCache` struct: `cache: Mutex<HashMap<String, (Robot, Instant)>>`, TTL of 1 hour. Method `pub async fn is_allowed(&self, client: &reqwest::Client, url: &str) -> Result<bool>` fetches robots.txt for the domain (once, then cached), checks if the specific URL path is allowed. Use `texting_robots::Robot::new("BambuMate/1.0", &body)`. Handle 404 (no robots.txt) as "all allowed". Method should also return crawl-delay from robots.txt if set.
    - `ScraperHttpClient` struct: wraps `reqwest::Client`, `RateLimiter`, `RobotsCache`. Constructor `pub fn new() -> Self` with default 1 req/sec rate limit and User-Agent "BambuMate/1.0". Public methods:
      - `pub async fn fetch_page(&self, url: &str) -> Result<String>` -- checks robots.txt first. If robots.txt disallows the URL, return `Err` with message "URL blocked by robots.txt: {url}" (do NOT proceed with the fetch). If allowed, wait for rate limit (using the **maximum** of the default min_interval and the crawl-delay from robots.txt, if any), fetch page, return HTML body. Set timeout of 30 seconds.
      - `pub fn html_to_text(html: &str) -> String` -- uses `html2text::from_read(html.as_bytes(), 120)` to convert HTML to plain text. This is a static utility method.

    **2. Create `src-tauri/src/scraper/prompts.rs`** with:
    - `pub fn filament_specs_json_schema() -> serde_json::Value` -- returns the JSON schema as specified in RESEARCH.md. All spec fields use `["integer", "null"]` or `["number", "null"]` types. Required fields: name, brand, material, plus all spec fields (with null allowed), plus confidence.
    - `pub fn build_extraction_prompt(filament_name: &str, page_text: &str) -> String` -- returns the extraction prompt from RESEARCH.md. Key rules: "Only extract values explicitly stated in the text below", "If a value is NOT present, return null", "Do NOT guess or use general knowledge", confidence scoring guidelines.

    **3. Create `src-tauri/src/scraper/extraction.rs`** with:
    - `pub async fn extract_specs(page_text: &str, filament_name: &str, provider: &str, model: &str, api_key: &str) -> Result<FilamentSpecs>` that:
      1. Builds prompt via `build_extraction_prompt(filament_name, page_text)`
      2. Gets JSON schema via `filament_specs_json_schema()`
      3. Calls the appropriate AI API based on `provider`:
         - For "claude": POST to `https://api.anthropic.com/v1/messages` with `output_config.format` containing `json_schema` (see RESEARCH.md for exact format). Headers: `x-api-key`, `anthropic-version: 2023-06-01`, `content-type: application/json`. Extract text from `response.content[0].text`.
         - For "openai": POST to `https://api.openai.com/v1/chat/completions` with `response_format` containing `json_schema` with `strict: true` (see RESEARCH.md). Header: `Authorization: Bearer {key}`. Extract text from `response.choices[0].message.content`.
         - For "kimi": Same as openai but POST to `https://api.moonshot.cn/v1/chat/completions`. Use standard JSON mode with prompt-based enforcement (not json_schema) as Kimi structured output support is unverified.
         - For "openrouter": Same as openai but POST to `https://openrouter.ai/api/v1/chat/completions`.
      4. Deserializes the response text into `FilamentSpecs` via `serde_json::from_str`
      5. Runs `validate_specs` on the result and attaches warnings (log them via tracing::warn but still return the specs)
      6. Returns the FilamentSpecs

    **4. Error handling for all API calls in extraction.rs (CRITICAL):**
    Handle API errors gracefully throughout `extract_specs`:
    - **Network timeouts:** Set a 60-second timeout on the reqwest client used for API calls. If the request times out, return `Err` with a descriptive message: `"LLM API timeout after 60s for provider '{provider}'"`. Do NOT panic.
    - **Non-200 status codes:** After sending the request, check the HTTP status. If not 2xx, read the response body (or as much as possible up to 1KB) and return `Err` with: `"LLM API error: {status_code} from {provider} - {truncated_body}"`. This covers rate limits (429), auth failures (401/403), server errors (500+).
    - **Invalid JSON from LLM:** If the LLM returns text that is not valid JSON, return `Err` with: `"Failed to parse LLM response as JSON: {serde_error}. Raw response (first 500 chars): {truncated_response}"`. Do NOT panic on malformed LLM output.
    - **Malformed JSON structure:** If the JSON parses but does not match the `FilamentSpecs` struct (missing required fields, wrong types), the `serde_json::from_str::<FilamentSpecs>` call will fail. Catch this and return `Err` with: `"LLM response JSON does not match FilamentSpecs schema: {serde_error}"`.
    - **Unsupported provider:** If `provider` does not match any of the 4 supported values, return `Err` with: `"Unsupported AI provider: '{provider}'. Supported: claude, openai, kimi, openrouter"`.
    - **All errors logged:** Before returning any `Err`, log the error via `tracing::error!` with the full error message for debugging.
    - **Never panic:** No `.unwrap()` or `.expect()` on API response parsing. All fallible operations use `?` or explicit match/map_err.

    **Important patterns to follow:**
    - Match the existing reqwest usage pattern from `commands/models.rs` (create client, build request, check status, parse response)
    - Use `tracing::info` and `tracing::warn` for logging, matching existing codebase style
    - Return `Result<FilamentSpecs, String>` for errors (matching Tauri command pattern) or use anyhow internally and convert at boundaries
    - The `api_key` parameter will be retrieved from keychain at the command layer (Plan 02), not here. This module receives the key as a parameter.
    - Do NOT create a new reqwest::Client per call in extraction.rs -- accept it as a parameter or use the ScraperHttpClient's inner client. Create a new client for each provider call is acceptable for now since extractions are infrequent.
  </action>
  <verify>
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo check -p bambumate-tauri` -- should compile with no errors.
    Run `cd /Users/michaelcurtis/Development/BambuMate && cargo test -p bambumate-tauri -- scraper` -- all tests pass.
    Write unit tests:
    - `prompts::filament_specs_json_schema()` returns valid JSON with all expected fields
    - `prompts::build_extraction_prompt("Polymaker PLA Pro", "some text")` contains the filament name and the source text and the anti-hallucination rules
    - `http_client::ScraperHttpClient::html_to_text("<h1>Hello</h1><p>World</p>")` returns text containing "Hello" and "World" without HTML tags
    - RateLimiter enforces delay (test that two sequential calls to same domain have at least min_interval between them -- use Instant::now() before/after)
    - **robots.txt blocking test:** Create a mock or test scenario where a robots.txt disallows a path. Call `fetch_page` with that URL and assert it returns an `Err` containing "blocked by robots.txt". This can be tested by directly calling `RobotsCache::is_allowed` with a known-disallowed path against a Robot built from a test robots.txt string (e.g., `Robot::new("BambuMate/1.0", b"User-agent: *\nDisallow: /private/")` and checking that `/private/page` returns false).
    - **crawl-delay respect test:** Build a Robot from a robots.txt string containing `Crawl-delay: 5`. Verify that `fetch_page` (or the rate limiter integration) uses `max(default_interval, crawl_delay)` -- i.e., 5 seconds, not 1 second. This can be tested by checking that `RobotsCache` correctly extracts and returns the crawl-delay value, and that `ScraperHttpClient::fetch_page` passes it to the rate limiter.
  </verify>
  <done>
    - ScraperHttpClient fetches pages with rate limiting (1 req/sec per domain) and robots.txt checking
    - fetch_page returns Err when robots.txt disallows the URL (does not proceed with fetch)
    - crawl-delay from robots.txt is respected when higher than default 1-second interval
    - html_to_text converts HTML to clean plain text
    - LLM extraction supports 4 providers (claude, openai, kimi, openrouter) with structured output schemas
    - Extraction prompt explicitly forbids hallucination ("return null, do NOT guess")
    - API errors handled gracefully: timeouts return Err, non-200 returns Err with status+body, invalid JSON returns Err with parse error, malformed responses return Err with schema mismatch details
    - All errors logged via tracing::error, no panics on malformed LLM output
    - All code compiles and tests pass
  </done>
</task>

</tasks>

<verification>
1. `cargo check -p bambumate-tauri` compiles cleanly
2. `cargo test -p bambumate-tauri -- scraper` runs all scraper module tests and they pass
3. The scraper module structure exists: mod.rs, types.rs, validation.rs, http_client.rs, extraction.rs, prompts.rs
4. FilamentSpecs round-trips through serde_json (serialize then deserialize produces equal struct)
5. Validation catches PLA at 350C (warning produced) and passes PLA at 210C (no warning)
6. robots.txt disallow returns Err from fetch_page (URL is not fetched)
7. crawl-delay from robots.txt is used when higher than default rate limit interval
8. LLM extraction returns descriptive Err (not panic) for network timeout, non-200 status, invalid JSON, and malformed response structure
</verification>

<success_criteria>
- scraper module compiles and all tests pass
- FilamentSpecs type matches the JSON schema used for LLM extraction
- Physical constraint validation covers all 10 material types with per-material ranges
- HTTP client rate-limits to 1 req/sec per domain
- robots.txt is checked before every page fetch; blocked URLs return error without fetching
- crawl-delay from robots.txt overrides default interval when higher
- LLM extraction supports all 4 configured AI providers
- All API call failures (timeout, HTTP errors, bad JSON, schema mismatch) return descriptive errors, never panic
- No hardcoded API keys anywhere -- keys passed as parameters
</success_criteria>

<output>
After completion, create `.planning/phases/03-filament-scraping/03-01-SUMMARY.md`
</output>
