---
phase: 06-ai-print-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src-tauri/Cargo.toml
  - src-tauri/src/lib.rs
  - src-tauri/src/analyzer/mod.rs
  - src-tauri/src/analyzer/types.rs
  - src-tauri/src/analyzer/image_prep.rs
  - src-tauri/src/analyzer/vision.rs
  - src-tauri/src/analyzer/prompts.rs
autonomous: true

must_haves:
  truths:
    - "Image bytes can be loaded, resized to max 1024px, and encoded to base64 JPEG"
    - "Vision API calls return structured DefectReport with defect types, severity, and confidence"
    - "All 4 AI providers (Claude, OpenAI, Kimi, OpenRouter) support vision requests"
  artifacts:
    - path: "src-tauri/src/analyzer/image_prep.rs"
      provides: "Image loading, resizing, and base64 encoding"
      exports: ["prepare_image"]
    - path: "src-tauri/src/analyzer/vision.rs"
      provides: "Vision API calls for all providers"
      exports: ["analyze_image"]
    - path: "src-tauri/src/analyzer/types.rs"
      provides: "AnalysisRequest, DefectReport types"
      contains: "struct DefectReport"
    - path: "src-tauri/src/analyzer/prompts.rs"
      provides: "Defect analysis prompt and JSON schema"
      exports: ["build_defect_analysis_prompt", "defect_report_schema"]
  key_links:
    - from: "src-tauri/src/analyzer/vision.rs"
      to: "src-tauri/src/scraper/extraction.rs"
      via: "Same provider pattern (call_claude, call_openai, etc.)"
      pattern: "call_claude_vision|call_openai_vision"
    - from: "src-tauri/src/analyzer/image_prep.rs"
      to: "image crate"
      via: "DynamicImage::resize"
      pattern: "image::load_from_memory"
---

<objective>
Create the analyzer module with image preparation and vision API integration for all 4 AI providers.

Purpose: This is the foundation for AI-powered print defect analysis. The image_prep module handles resizing photos to 1024px max (controlling API costs), and the vision module extends the existing extraction.rs pattern to send images with structured output prompts.

Output:
- `src-tauri/src/analyzer/` module with image_prep.rs, vision.rs, prompts.rs, types.rs
- Dependencies: `image 0.25`, `base64 0.22` added to Cargo.toml
</objective>

<execution_context>
@/Users/michaelcurtis/.claude/get-shit-done/workflows/execute-plan.md
@/Users/michaelcurtis/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-ai-print-analysis/06-RESEARCH.md

# Existing AI provider pattern to extend
@src-tauri/src/scraper/extraction.rs

# Types for defect detection (DetectedDefect is already defined)
@src-tauri/src/mapper/types.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies and create analyzer types</name>
  <files>
    src-tauri/Cargo.toml
    src-tauri/src/lib.rs
    src-tauri/src/analyzer/mod.rs
    src-tauri/src/analyzer/types.rs
  </files>
  <action>
1. Add to src-tauri/Cargo.toml:
   ```toml
   image = "0.25"
   base64 = "0.22"
   ```

2. Add `pub mod analyzer;` to src-tauri/src/lib.rs

3. Create src-tauri/src/analyzer/mod.rs with:
   ```rust
   //! AI vision analysis for 3D print defect detection.
   pub mod types;
   pub mod image_prep;
   pub mod vision;
   pub mod prompts;

   pub use types::*;
   pub use image_prep::prepare_image;
   pub use vision::analyze_image;
   ```

4. Create src-tauri/src/analyzer/types.rs with:
   - `DefectReport` struct with:
     - `defects: Vec<DetectedDefect>` (import from crate::mapper::types)
     - `overall_quality: String` (enum: excellent/good/acceptable/poor/failed)
     - `notes: Option<String>`
   - `AnalysisRequest` struct with:
     - `image_bytes: Vec<u8>`
     - `profile_path: Option<String>` (for loading current settings)
     - `material_type: Option<String>` (for safe-range enforcement)
   - `AnalysisResult` struct with:
     - `defect_report: DefectReport`
     - `recommendations: Vec<Recommendation>` (from mapper::types)
     - `conflicts: Vec<Conflict>` (from mapper::types)
     - `profile_context: HashMap<String, f32>` (current values used)

   All types should derive Serialize/Deserialize for Tauri IPC.
  </action>
  <verify>
    `cd /Users/michaelcurtis/Development/BambuMate/src-tauri && cargo check`
  </verify>
  <done>
    - Cargo.toml has image and base64 dependencies
    - analyzer module declared in lib.rs
    - Types compile and can import DetectedDefect from mapper
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement image preparation (resize + base64)</name>
  <files>
    src-tauri/src/analyzer/image_prep.rs
  </files>
  <action>
Create src-tauri/src/analyzer/image_prep.rs:

```rust
//! Image loading, resizing, and base64 encoding for vision APIs.
//!
//! All images are resized to max 1024px on longest edge to control
//! API costs (requirement FNDN-05).

use base64::{engine::general_purpose::STANDARD, Engine};
use image::{DynamicImage, ImageFormat};
use std::io::Cursor;
use tracing::info;

/// Maximum dimension (width or height) for images sent to vision APIs.
/// Requirement FNDN-05: Photos resized to max 1024px before sending.
pub const MAX_IMAGE_DIMENSION: u32 = 1024;

/// Minimum dimension for valid analysis (too small = poor detection).
pub const MIN_IMAGE_DIMENSION: u32 = 200;

/// Prepare an image for vision API: load, validate, resize, encode.
///
/// # Arguments
/// * `image_bytes` - Raw image bytes (JPEG, PNG, WebP, etc.)
///
/// # Returns
/// Base64-encoded JPEG string ready for API payload.
///
/// # Errors
/// - Image cannot be decoded
/// - Image too small (< 200px on shortest side)
pub fn prepare_image(image_bytes: &[u8]) -> Result<String, String> {
    // Load image
    let img = image::load_from_memory(image_bytes)
        .map_err(|e| format!("Failed to load image: {}. Ensure it's a valid JPEG/PNG/WebP.", e))?;

    let (width, height) = (img.width(), img.height());
    info!("Loaded image: {}x{}", width, height);

    // Validate minimum size
    let min_side = width.min(height);
    if min_side < MIN_IMAGE_DIMENSION {
        return Err(format!(
            "Image too small for reliable analysis: {}x{}. Minimum dimension is {}px.",
            width, height, MIN_IMAGE_DIMENSION
        ));
    }

    // Resize if needed (maintain aspect ratio)
    let resized = resize_if_needed(img, MAX_IMAGE_DIMENSION);
    info!("Resized to: {}x{}", resized.width(), resized.height());

    // Encode to JPEG
    let jpeg_bytes = encode_to_jpeg(&resized)?;
    info!("Encoded to JPEG: {} bytes", jpeg_bytes.len());

    // Base64 encode
    let base64_string = STANDARD.encode(&jpeg_bytes);

    Ok(base64_string)
}

/// Resize image if either dimension exceeds max, maintaining aspect ratio.
fn resize_if_needed(img: DynamicImage, max_dimension: u32) -> DynamicImage {
    let (width, height) = (img.width(), img.height());

    if width <= max_dimension && height <= max_dimension {
        return img;
    }

    // Calculate scale factor
    let scale = max_dimension as f32 / width.max(height) as f32;
    let new_width = (width as f32 * scale) as u32;
    let new_height = (height as f32 * scale) as u32;

    img.resize(new_width, new_height, image::imageops::FilterType::Lanczos3)
}

/// Encode DynamicImage to JPEG bytes.
fn encode_to_jpeg(img: &DynamicImage) -> Result<Vec<u8>, String> {
    let mut buffer = Cursor::new(Vec::new());
    img.write_to(&mut buffer, ImageFormat::Jpeg)
        .map_err(|e| format!("Failed to encode image to JPEG: {}", e))?;
    Ok(buffer.into_inner())
}

/// Get the media type for vision API payloads.
pub fn image_media_type() -> &'static str {
    "image/jpeg"
}

#[cfg(test)]
mod tests {
    use super::*;

    // 2x2 red PNG for testing
    const TINY_PNG: &[u8] = &[
        0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A,
        0x00, 0x00, 0x00, 0x0D, 0x49, 0x48, 0x44, 0x52,
        0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x02,
        0x08, 0x02, 0x00, 0x00, 0x00, 0xFD, 0xD4, 0x9A,
        0x73, 0x00, 0x00, 0x00, 0x14, 0x49, 0x44, 0x41,
        0x54, 0x78, 0x9C, 0x62, 0xF8, 0xCF, 0xC0, 0x00,
        0x00, 0x00, 0x00, 0xFF, 0xFF, 0x03, 0x00, 0x06,
        0x00, 0x01, 0x02, 0x39, 0x54, 0xA6, 0x00, 0x00,
        0x00, 0x00, 0x49, 0x45, 0x4E, 0x44, 0xAE, 0x42,
        0x60, 0x82,
    ];

    #[test]
    fn test_prepare_image_rejects_too_small() {
        let result = prepare_image(TINY_PNG);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("too small"));
    }

    #[test]
    fn test_prepare_image_rejects_invalid() {
        let result = prepare_image(b"not an image");
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Failed to load"));
    }

    #[test]
    fn test_resize_if_needed_no_resize() {
        // Create a small test image
        let img = DynamicImage::new_rgb8(500, 300);
        let resized = resize_if_needed(img, 1024);
        assert_eq!(resized.width(), 500);
        assert_eq!(resized.height(), 300);
    }

    #[test]
    fn test_resize_if_needed_resize_width() {
        let img = DynamicImage::new_rgb8(2000, 1000);
        let resized = resize_if_needed(img, 1024);
        assert_eq!(resized.width(), 1024);
        assert_eq!(resized.height(), 512);
    }

    #[test]
    fn test_resize_if_needed_resize_height() {
        let img = DynamicImage::new_rgb8(1000, 2000);
        let resized = resize_if_needed(img, 1024);
        assert_eq!(resized.width(), 512);
        assert_eq!(resized.height(), 1024);
    }

    #[test]
    fn test_image_media_type() {
        assert_eq!(image_media_type(), "image/jpeg");
    }
}
```
  </action>
  <verify>
    `cd /Users/michaelcurtis/Development/BambuMate/src-tauri && cargo test image_prep`
  </verify>
  <done>
    - prepare_image resizes images exceeding 1024px
    - prepare_image rejects images smaller than 200px
    - Output is valid base64-encoded JPEG
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement vision API calls and prompts</name>
  <files>
    src-tauri/src/analyzer/prompts.rs
    src-tauri/src/analyzer/vision.rs
  </files>
  <action>
1. Create src-tauri/src/analyzer/prompts.rs:

```rust
//! Prompts and schemas for defect analysis vision API calls.

use std::collections::HashMap;

/// JSON schema for structured defect report output.
/// Matches the DetectedDefect type from mapper::types.
pub fn defect_report_schema() -> serde_json::Value {
    serde_json::json!({
        "type": "object",
        "properties": {
            "defects": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "defect_type": {
                            "type": "string",
                            "enum": ["stringing", "warping", "layer_adhesion",
                                    "elephants_foot", "under_extrusion",
                                    "over_extrusion", "z_banding"]
                        },
                        "severity": {
                            "type": "number",
                            "description": "0.0-1.0 scale: 0.3=minor, 0.5=noticeable, 0.7=significant, 0.9=severe"
                        },
                        "confidence": {
                            "type": "number",
                            "description": "0.0-1.0 confidence in detection accuracy"
                        }
                    },
                    "required": ["defect_type", "severity", "confidence"],
                    "additionalProperties": false
                }
            },
            "overall_quality": {
                "type": "string",
                "enum": ["excellent", "good", "acceptable", "poor", "failed"]
            },
            "notes": {
                "type": ["string", "null"],
                "description": "Brief observation about the print (optional)"
            }
        },
        "required": ["defects", "overall_quality"],
        "additionalProperties": false
    })
}

/// Build the defect analysis prompt with current profile context.
///
/// # Arguments
/// * `current_settings` - Current profile parameter values
/// * `material_type` - Material type string (e.g., "PLA", "PETG")
pub fn build_defect_analysis_prompt(
    current_settings: &HashMap<String, f32>,
    material_type: &str,
) -> String {
    let nozzle_temp = current_settings.get("nozzle_temperature").unwrap_or(&200.0);
    let bed_temp = current_settings.get("cool_plate_temp")
        .or_else(|| current_settings.get("hot_plate_temp"))
        .unwrap_or(&60.0);
    let retraction = current_settings.get("filament_retraction_length").unwrap_or(&0.8);
    let flow = current_settings.get("filament_flow_ratio").unwrap_or(&1.0);

    format!(r#"Analyze this 3D print photo for defects.

Current print settings:
- Material: {material}
- Nozzle temperature: {nozzle_temp}C
- Bed temperature: {bed_temp}C
- Retraction: {retraction}mm
- Flow ratio: {flow}

Identify any defects from this list:
- stringing: Fine threads/wisps between parts or during travel moves
- warping: Corners or edges lifting from the print bed
- layer_adhesion: Weak bonds between layers, visible gaps, delamination
- elephants_foot: First layer(s) bulging outward, wider than intended
- under_extrusion: Gaps in walls, missing material, thin/weak layers
- over_extrusion: Blobs, rough surfaces, material oozing, dimensional inaccuracy
- z_banding: Horizontal lines/ridges at regular intervals on vertical surfaces

For each defect found, rate:
- severity: 0.3=minor/cosmetic only, 0.5=noticeable but functional, 0.7=significant quality issue, 0.9=severe/print failure
- confidence: How certain you are this defect is actually present (not lighting/angle artifact)

Rate overall print quality:
- excellent: No visible defects, professional quality
- good: Minor cosmetic issues only
- acceptable: Some defects but functional
- poor: Significant quality issues
- failed: Print unusable

If no defects are visible, return an empty defects array and rate as excellent/good.
If image is unclear or not a 3D print, note this in the notes field."#,
        material = material_type,
        nozzle_temp = nozzle_temp,
        bed_temp = bed_temp,
        retraction = retraction,
        flow = flow,
    )
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_defect_report_schema_structure() {
        let schema = defect_report_schema();
        assert_eq!(schema["type"], "object");
        assert!(schema["properties"]["defects"].is_object());
        assert!(schema["properties"]["overall_quality"].is_object());
    }

    #[test]
    fn test_build_prompt_includes_settings() {
        let mut settings = HashMap::new();
        settings.insert("nozzle_temperature".to_string(), 215.0);
        settings.insert("cool_plate_temp".to_string(), 55.0);

        let prompt = build_defect_analysis_prompt(&settings, "PLA");
        assert!(prompt.contains("215"));
        assert!(prompt.contains("55"));
        assert!(prompt.contains("PLA"));
    }

    #[test]
    fn test_build_prompt_uses_defaults() {
        let settings = HashMap::new();
        let prompt = build_defect_analysis_prompt(&settings, "PETG");
        assert!(prompt.contains("200")); // default nozzle temp
        assert!(prompt.contains("60")); // default bed temp
    }
}
```

2. Create src-tauri/src/analyzer/vision.rs:

```rust
//! Vision API calls for defect analysis across all supported providers.
//!
//! Extends the pattern from scraper/extraction.rs to support image content.

use std::collections::HashMap;
use std::time::Duration;

use serde_json;
use tracing::{error, info};

use super::image_prep::{image_media_type, prepare_image};
use super::prompts::{build_defect_analysis_prompt, defect_report_schema};
use super::types::DefectReport;
use crate::mapper::types::DetectedDefect;

/// Analyze an image for print defects using the specified AI provider.
///
/// # Arguments
/// * `image_bytes` - Raw image bytes (will be resized and encoded)
/// * `current_settings` - Current profile parameter values for context
/// * `material_type` - Material type string (e.g., "PLA", "PETG")
/// * `provider` - AI provider: "claude", "openai", "kimi", or "openrouter"
/// * `model` - Model identifier
/// * `api_key` - API key for the provider
///
/// # Returns
/// DefectReport with detected defects, overall quality, and notes.
pub async fn analyze_image(
    image_bytes: &[u8],
    current_settings: &HashMap<String, f32>,
    material_type: &str,
    provider: &str,
    model: &str,
    api_key: &str,
) -> Result<DefectReport, String> {
    // Prepare image (resize + base64)
    let base64_image = prepare_image(image_bytes)?;

    let prompt = build_defect_analysis_prompt(current_settings, material_type);
    let schema = defect_report_schema();

    info!(
        "Analyzing print photo using provider '{}' model '{}'",
        provider, model
    );

    // Call the appropriate vision API
    let response_text = match provider {
        "claude" => call_claude_vision(api_key, model, &prompt, &base64_image, &schema).await?,
        "openai" => call_openai_vision(api_key, model, &prompt, &base64_image, &schema).await?,
        "kimi" => call_kimi_vision(api_key, model, &prompt, &base64_image).await?,
        "openrouter" => call_openrouter_vision(api_key, model, &prompt, &base64_image, &schema).await?,
        _ => {
            let msg = format!(
                "Unsupported AI provider: '{}'. Supported: claude, openai, kimi, openrouter",
                provider
            );
            error!("{}", msg);
            return Err(msg);
        }
    };

    // Parse response
    let report = parse_defect_report(&response_text)?;

    info!(
        "Analysis complete: {} defects found, overall quality: {}",
        report.defects.len(),
        report.overall_quality
    );

    Ok(report)
}

/// Parse the raw JSON response into a DefectReport.
fn parse_defect_report(response_text: &str) -> Result<DefectReport, String> {
    let json: serde_json::Value = serde_json::from_str(response_text).map_err(|e| {
        let truncated = if response_text.len() > 500 {
            format!("{}...", &response_text[..500])
        } else {
            response_text.to_string()
        };
        format!("Failed to parse defect report JSON: {}. Response: {}", e, truncated)
    })?;

    // Parse defects array
    let defects: Vec<DetectedDefect> = json["defects"]
        .as_array()
        .ok_or("Missing 'defects' array")?
        .iter()
        .filter_map(|d| {
            Some(DetectedDefect {
                defect_type: d["defect_type"].as_str()?.to_string(),
                severity: d["severity"].as_f64()? as f32,
                confidence: d["confidence"].as_f64()? as f32,
            })
        })
        .collect();

    let overall_quality = json["overall_quality"]
        .as_str()
        .ok_or("Missing 'overall_quality' field")?
        .to_string();

    let notes = json["notes"].as_str().map(|s| s.to_string());

    Ok(DefectReport {
        defects,
        overall_quality,
        notes,
    })
}

/// Build a reqwest client with timeout for vision API calls.
fn build_api_client() -> Result<reqwest::Client, String> {
    reqwest::Client::builder()
        .timeout(Duration::from_secs(90)) // Vision calls take longer
        .build()
        .map_err(|e| format!("Failed to build HTTP client: {}", e))
}

/// Handle API response status and extract body.
async fn handle_api_response(response: reqwest::Response, provider: &str) -> Result<String, String> {
    let status = response.status();
    if !status.is_success() {
        let body = response.text().await.unwrap_or_else(|_| "<failed to read>".to_string());
        let truncated = if body.len() > 1024 {
            format!("{}...", &body[..1024])
        } else {
            body
        };
        return Err(format!("Vision API error: {} from {} - {}", status, provider, truncated));
    }
    response.text().await.map_err(|e| format!("Failed to read response: {}", e))
}

/// Call Claude Vision API.
async fn call_claude_vision(
    api_key: &str,
    model: &str,
    prompt: &str,
    base64_image: &str,
    schema: &serde_json::Value,
) -> Result<String, String> {
    let client = build_api_client()?;

    let body = serde_json::json!({
        "model": model,
        "max_tokens": 1024,
        "messages": [{
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": image_media_type(),
                        "data": base64_image
                    }
                },
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        }],
        "output_config": {
            "format": {
                "type": "json_schema",
                "schema": schema
            }
        }
    });

    let response = client
        .post("https://api.anthropic.com/v1/messages")
        .header("x-api-key", api_key)
        .header("anthropic-version", "2023-06-01")
        .header("content-type", "application/json")
        .json(&body)
        .send()
        .await
        .map_err(|e| {
            if e.is_timeout() {
                "Vision API timeout after 90s for provider 'claude'".to_string()
            } else {
                format!("Vision API request failed for claude: {}", e)
            }
        })?;

    let body_text = handle_api_response(response, "claude").await?;

    // Parse Claude response wrapper
    let resp_json: serde_json::Value = serde_json::from_str(&body_text)
        .map_err(|e| format!("Failed to parse Claude response: {}", e))?;

    resp_json["content"][0]["text"]
        .as_str()
        .map(|s| s.to_string())
        .ok_or_else(|| "No text content in Claude vision response".to_string())
}

/// Call OpenAI Vision API.
async fn call_openai_vision(
    api_key: &str,
    model: &str,
    prompt: &str,
    base64_image: &str,
    schema: &serde_json::Value,
) -> Result<String, String> {
    let client = build_api_client()?;

    let body = serde_json::json!({
        "model": model,
        "max_tokens": 1024,
        "messages": [{
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": format!("data:{};base64,{}", image_media_type(), base64_image),
                        "detail": "low"  // Cost-efficient for defect detection
                    }
                },
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        }],
        "response_format": {
            "type": "json_schema",
            "json_schema": {
                "name": "defect_report",
                "strict": true,
                "schema": schema
            }
        }
    });

    let response = client
        .post("https://api.openai.com/v1/chat/completions")
        .header("Authorization", format!("Bearer {}", api_key))
        .header("content-type", "application/json")
        .json(&body)
        .send()
        .await
        .map_err(|e| {
            if e.is_timeout() {
                "Vision API timeout after 90s for provider 'openai'".to_string()
            } else {
                format!("Vision API request failed for openai: {}", e)
            }
        })?;

    let body_text = handle_api_response(response, "openai").await?;

    let resp_json: serde_json::Value = serde_json::from_str(&body_text)
        .map_err(|e| format!("Failed to parse OpenAI response: {}", e))?;

    resp_json["choices"][0]["message"]["content"]
        .as_str()
        .map(|s| s.to_string())
        .ok_or_else(|| "No content in OpenAI vision response".to_string())
}

/// Call Kimi Vision API (uses json_object mode).
async fn call_kimi_vision(
    api_key: &str,
    model: &str,
    prompt: &str,
    base64_image: &str,
) -> Result<String, String> {
    let client = build_api_client()?;

    // Kimi may not support vision - include image URL in content
    // If Kimi doesn't support vision, this will return an error
    let body = serde_json::json!({
        "model": model,
        "max_tokens": 1024,
        "messages": [{
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": format!("data:{};base64,{}", image_media_type(), base64_image)
                    }
                },
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        }],
        "response_format": {
            "type": "json_object"
        }
    });

    let response = client
        .post("https://api.moonshot.cn/v1/chat/completions")
        .header("Authorization", format!("Bearer {}", api_key))
        .header("content-type", "application/json")
        .json(&body)
        .send()
        .await
        .map_err(|e| {
            if e.is_timeout() {
                "Vision API timeout after 90s for provider 'kimi'".to_string()
            } else {
                format!("Vision API request failed for kimi: {}", e)
            }
        })?;

    let body_text = handle_api_response(response, "kimi").await?;

    let resp_json: serde_json::Value = serde_json::from_str(&body_text)
        .map_err(|e| format!("Failed to parse Kimi response: {}", e))?;

    resp_json["choices"][0]["message"]["content"]
        .as_str()
        .map(|s| s.to_string())
        .ok_or_else(|| "No content in Kimi vision response".to_string())
}

/// Call OpenRouter Vision API.
async fn call_openrouter_vision(
    api_key: &str,
    model: &str,
    prompt: &str,
    base64_image: &str,
    schema: &serde_json::Value,
) -> Result<String, String> {
    let client = build_api_client()?;

    let body = serde_json::json!({
        "model": model,
        "max_tokens": 1024,
        "messages": [{
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": format!("data:{};base64,{}", image_media_type(), base64_image),
                        "detail": "low"
                    }
                },
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        }],
        "response_format": {
            "type": "json_schema",
            "json_schema": {
                "name": "defect_report",
                "strict": true,
                "schema": schema
            }
        }
    });

    let response = client
        .post("https://openrouter.ai/api/v1/chat/completions")
        .header("Authorization", format!("Bearer {}", api_key))
        .header("content-type", "application/json")
        .json(&body)
        .send()
        .await
        .map_err(|e| {
            if e.is_timeout() {
                "Vision API timeout after 90s for provider 'openrouter'".to_string()
            } else {
                format!("Vision API request failed for openrouter: {}", e)
            }
        })?;

    let body_text = handle_api_response(response, "openrouter").await?;

    let resp_json: serde_json::Value = serde_json::from_str(&body_text)
        .map_err(|e| format!("Failed to parse OpenRouter response: {}", e))?;

    resp_json["choices"][0]["message"]["content"]
        .as_str()
        .map(|s| s.to_string())
        .ok_or_else(|| "No content in OpenRouter vision response".to_string())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_parse_defect_report_valid() {
        let json = r#"{
            "defects": [
                {"defect_type": "stringing", "severity": 0.6, "confidence": 0.85}
            ],
            "overall_quality": "acceptable",
            "notes": "Minor stringing visible"
        }"#;

        let report = parse_defect_report(json).unwrap();
        assert_eq!(report.defects.len(), 1);
        assert_eq!(report.defects[0].defect_type, "stringing");
        assert_eq!(report.overall_quality, "acceptable");
        assert_eq!(report.notes, Some("Minor stringing visible".to_string()));
    }

    #[test]
    fn test_parse_defect_report_no_defects() {
        let json = r#"{
            "defects": [],
            "overall_quality": "excellent",
            "notes": null
        }"#;

        let report = parse_defect_report(json).unwrap();
        assert!(report.defects.is_empty());
        assert_eq!(report.overall_quality, "excellent");
        assert!(report.notes.is_none());
    }

    #[test]
    fn test_parse_defect_report_missing_quality() {
        let json = r#"{"defects": []}"#;
        let result = parse_defect_report(json);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("overall_quality"));
    }

    #[tokio::test]
    async fn test_analyze_image_unsupported_provider() {
        let result = analyze_image(
            &[0; 100],
            &HashMap::new(),
            "PLA",
            "invalid_provider",
            "model",
            "key"
        ).await;
        // Will fail on image prep first, but that's fine for this test
        assert!(result.is_err());
    }
}
```
  </action>
  <verify>
    `cd /Users/michaelcurtis/Development/BambuMate/src-tauri && cargo test analyzer`
  </verify>
  <done>
    - Vision API calls implemented for all 4 providers
    - Prompts include current profile settings for context
    - JSON schema matches DetectedDefect structure
    - All tests pass
  </done>
</task>

</tasks>

<verification>
1. `cd /Users/michaelcurtis/Development/BambuMate/src-tauri && cargo build` - Full build succeeds
2. `cd /Users/michaelcurtis/Development/BambuMate/src-tauri && cargo test analyzer` - All analyzer tests pass
3. Verify analyzer module exports: `cargo doc --open` and check analyzer module docs
</verification>

<success_criteria>
- image and base64 dependencies added to Cargo.toml
- analyzer module created with types.rs, image_prep.rs, vision.rs, prompts.rs
- prepare_image resizes images to max 1024px and returns base64 JPEG
- analyze_image calls vision API and returns DefectReport
- All 4 providers (Claude, OpenAI, Kimi, OpenRouter) supported
- Unit tests pass for image prep and JSON parsing
</success_criteria>

<output>
After completion, create `.planning/phases/06-ai-print-analysis/06-01-SUMMARY.md`
</output>
